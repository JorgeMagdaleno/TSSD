{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ec4881-f398-43a7-87a8-18c73558beb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlwt in ./.venv/lib/python3.9/site-packages (1.3.0)\n",
      "Requirement already satisfied: openpyxl in ./.venv/lib/python3.9/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in ./.venv/lib/python3.9/site-packages (from openpyxl) (2.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xlwt openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5998a746-e1f6-4a75-aa48-75508d33ba9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgemagdaleno/PycharmProjects/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Recuperar la página web\n",
    "url = \"https://www.bloghemia.com/2019/05/noam-chomsky-michel-foucault-debate.html\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extraer el encabezado y armar el dataframe\n",
    "header = soup.head\n",
    "header_tags = [tag.name for tag in header.find_all()]\n",
    "header_contents = [tag.string for tag in header.find_all()]\n",
    "\n",
    "df_header = pd.DataFrame({'Tag': header_tags, 'Content': header_contents})\n",
    "\n",
    "# Agrupar por tipo de etiqueta\n",
    "grouped_df = df_header.groupby('Tag').agg(lambda x: ', '.join(x.dropna())).reset_index()\n",
    "\n",
    "# Escribir los estilos en un archivo CSS\n",
    "inline_styles = soup.find_all('style')\n",
    "with open('Wrangle/styles.css', 'w') as f:\n",
    "    for style in inline_styles:\n",
    "        f.write(style.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d2676b-ad98-4d86-b920-20d71077b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Eliminar todos los inline scripts y anuncios\n",
    "for script in soup.find_all('script'):\n",
    "    script.decompose()\n",
    "\n",
    "for ad in soup.find_all(class_=lambda x: x and 'ad' in x):\n",
    "    ad.decompose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30c0eb2e-26f3-4672-9ab2-eb8fcd8c7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Obtener todos los metadatos y escribirlos a un archivo Excel\n",
    "meta_tags = soup.find_all('meta')\n",
    "meta_data = [{'name': tag.get('name'), 'content': tag.get('content')} for tag in meta_tags]\n",
    "df_meta = pd.DataFrame(meta_data)\n",
    "df_meta.to_excel('Wrangle/metadata.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4430f91-ee49-4b0d-b61c-2a15bc62b167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de intervenciones encontradas: 115\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Definir el patrón: que el texto inicie con ELDERS, CHOMSKY o FOUCAULT seguido de dos puntos.\n",
    "pattern = re.compile(r'^(ELDERS|CHOMSKY|FOUCAULT):', re.IGNORECASE)\n",
    "\n",
    "# Buscar todos los spans y filtrar aquellos que coincidan con el patrón\n",
    "intervenciones = [span for span in soup.find_all('span') \n",
    "                   if pattern.match(span.get_text(strip=True))]\n",
    "\n",
    "# Verificar la cantidad de intervenciones encontradas\n",
    "print(\"Número de intervenciones encontradas:\", len(intervenciones))\n",
    "\n",
    "# Generar el HTML mínimo con las intervenciones\n",
    "html_minimo = \"<html><head><meta charset='UTF-8'><title>{}</title></head><body>{}</body></html>\".format(\n",
    "    soup.title.string if soup.title else \"Debate\", \n",
    "    '</br>'.join(str(intervencion) for intervencion in intervenciones)\n",
    ")\n",
    "\n",
    "with open('Wrangle/debate_minimo.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(html_minimo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ea70b1-2c70-487f-bca3-8c92f2128d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir el patrón: el texto debe iniciar con \"ELDERS\", \"CHOMSKY\" o \"FOUCAULT\" seguido de dos puntos\n",
    "pattern = re.compile(r'^(ELDERS|CHOMSKY|FOUCAULT):')\n",
    "\n",
    "# Buscar todos los spans y filtrar los que cumplan con el patrón\n",
    "intervenciones_spans = [\n",
    "    span for span in soup.find_all('span')\n",
    "    if pattern.match(span.get_text(strip=True))\n",
    "]\n",
    "\n",
    "# Preparar una lista para almacenar la información: nombre del interlocutor y su intervención\n",
    "data = []\n",
    "for span in intervenciones_spans:\n",
    "    # Obtener el texto limpio del span\n",
    "    texto = span.get_text(strip=True)\n",
    "    # Separar el nombre del interlocutor y el contenido usando partition (divide en 3 partes)\n",
    "    speaker, sep, content = texto.partition(':')\n",
    "    data.append({\n",
    "        \"speaker\": speaker.strip(),\n",
    "        \"intervention\": content.strip()\n",
    "    })\n",
    "\n",
    "# Crear un DataFrame con la información\n",
    "df_interv = pd.DataFrame(data)\n",
    "\n",
    "# Agrupar las intervenciones por interlocutor y convertirlas en una lista\n",
    "df_grouped = df_interv.groupby(\"speaker\", as_index=False)[\"intervention\"].agg(list)\n",
    "\n",
    "# Guardar el DataFrame agrupado en un archivo JSON\n",
    "df_grouped.to_json(\"Wrangle/interventions.json\", orient=\"records\", force_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee0cc56-b6fe-4d36-afe9-76aa54b33845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
